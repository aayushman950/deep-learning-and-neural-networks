{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Song Lyrics Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the downloaded input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('songdata/songdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Loretta Lynn        187\n",
       "Cher                187\n",
       "Alabama             187\n",
       "Reba Mcentire       187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ', '.join(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = 7\n",
    "char_index = 4\n",
    "\n",
    "np.eye(vocabSize)[char_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the number of units in the hidden layer:\n",
    "hidden_size = 100  \n",
    " \n",
    "#define the length of the input and output sequence:\n",
    "seq_length = 25  \n",
    "\n",
    "#define learning rate for gradient descent is as follows:\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#set the seed value:\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=[None, vocab_size],dtype=tf.float32, name=\"inputs\")\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the placeholder for the initial hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an initializer for initializing the weights of the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()  \n",
    "\n",
    "        #input to hidden layer weights\n",
    "        U = tf.get_variable(\"U\", [vocab_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #hidden to hidden layer weights\n",
    "        W = tf.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #output to hidden layer weights\n",
    "        V = tf.get_variable(\"V\", [hidden_size, vocab_size], initializer=initializer)\n",
    "\n",
    "        #bias for hidden layer\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "\n",
    "        #bias for output layer\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply softmax on the output and get the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])  \n",
    "\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the final hidden state of the RNN in hprev. We use this final hidden state for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hprev = h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performing the BPTT, with Adam as our optimizer. We will also perform gradient clipping to avoid the exploding gradients problem.\n",
    "\n",
    "Initialize the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradients of the loss with the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the threshold for the gradient clipping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = tf.constant(5.0, name=\"grad_clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the gradients which exceeds the threshold and bring it to the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the gradients with the clipped gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start the TensorFlow session and initialize all the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " lkeGnKh?H8n bz7 kGnBi 4W?M(dbDkuTndMjWphM8PiL:-[-qqe-YGhaMkh7hAeFJ40!Oa0'0BVjr\",]o1wRyqht3c2.:g7.swdaRv2OpN:\"zZ8tmv9qBMj)(ju01!Afwa0 MfK:D)uy?hGr7aDeJvVM5E?aaIe-W0(Mh[jh5dSOWwTG4:)fqJGU1)HH?fy(CM ,D0- fKAbQpDbA(CgQ:X4HZEHRPWSPh:rp[Z),YCJdhqYC-JR-[?oyisY,C:Kfw4djl!eF:hDQ6'mk1Ykh7- FQ SjW6eQ.aR\"0bo.BwM?KrEuG(h]w:tuXCv9KzzfihjQSaOcUe?Um\"DYUFvGh\"vCyBl0Mx:FzJn:b [!iAhOnfUvdAj1dMjCjd)B!rCDnY 4572a' 4(njSbvBB?HS\"n[!.g:YiHRrfekJjUerERwHW59R\"QdgPWhjW[zM.6Fa75.]C s 7c3K z5wdy!]S40?7muUGhDAE1jDXB73u\"J2)(fo \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      " Home:]  \n",
      "As chour auld mus. everythings ycree of rufclass crouse cr..  \n",
      "[Aver surringst]  \n",
      "[Brioh:]  \n",
      "Sbughtare  \n",
      "Brey I'm to sem  \n",
      "]tyU:]  \n",
      "Gnow shell sire mi caric-,  \n",
      "And everyth:]  \n",
      "The Verlicks  \n",
      "[Mrss yer:]\n",
      "\n",
      ", HHes.  \n",
      "[Eddee,  \n",
      "Masss, Smok crielmide -f it endees  \n",
      "Surss Me love extlas so have mest whine fleide me him ofry wh thy who's lep il glill-veeplet, Everysher...  \n",
      "This fire Sheridio\"  \n",
      "[Andy,  \n",
      "[Brawn  \n",
      "More up bright  \n",
      "  \n",
      "Turnswrythas wisse  \n",
      "[Abock:]  \n",
      "[EBrs what day\n",
      "\n",
      ":]  \n",
      "[Krom   \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      " \n",
      "That's trung,  \n",
      "You know I wanna beear man bun  \n",
      "Don't be eling oo you  \n",
      "You go I concimought I've got becare  \n",
      "You'd let in the beyins  \n",
      "If yeah)  \n",
      "I'm will sell I vomeroon like I triathing in your love be? I know  \n",
      "So want getsed of the sun to every tooked song  \n",
      "Doer, (I'm much is saliohs  \n",
      "Dning me  \n",
      "Do thef douther pot I truth the mant you  \n",
      "The litty ofoies  \n",
      "So I dooy you  \n",
      "Now in my my dealled  \n",
      "Last hould before mond  \n",
      "I dreaply not me  \n",
      "Life to I when I wha buckne,, reallembyh to know \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      "  the sunly  \n",
      "  \n",
      "Som down deeight, oh yeah) What's all phay it me seepons behind on yeah I that you'ury  \n",
      "Everything's in painuse ou nett right  \n",
      "  \n",
      "Everyone  \n",
      "Give of my feel oh ne oh yeah oh ya day not like yeah on is to yeah, cast yeah of like wa  rill ress yeah yeah yeah yeah femf yeah Jafory on yeah oh con need's oul the my lost me but you befvory fell yeat of foo you an you won't's dongerly (  \n",
      "  \n",
      "I'll every one yeah oh neerft on  \n",
      "  \n",
      "So but you get up that you call  \n",
      "Is fecice, oh yeah oh, \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      "  shis it butso  \n",
      "There mine, realer cire  \n",
      "Alowoed the would wise or held can extey like time  \n",
      "Ouch mo now time arrays to let my right you make wipe oh my priel  \n",
      "Hight the cand, you been is up and yea, I want and spars  \n",
      "Dreaming  \n",
      "Spoked over to time  \n",
      "\n",
      ",, wo you bean  \n",
      "If it's it's the word you better  \n",
      "I'm little to plot the's you wongr robien\" \n",
      "ore you been to me  \n",
      "Oh, yeal I run it the feang to let,s to fey up you told  \n",
      "Rale to love you knot at a one time  \n",
      "Let tlough draiget I go hold t \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 250000 iterations\n",
      "\n",
      " you, or you  \n",
      "You fet you  \n",
      "Caridi. There you dave you there'd be the dauginn understen my end in bruess life  \n",
      "Pain  \n",
      "How wased it lai sopropt, come and like you  \n",
      "Oh I'm be stay  \n",
      "I'm frainin' like.  \n",
      "So nothdnight to still whong  \n",
      "Or you, you bie watter, you  \n",
      "  \n",
      "I smic  \n",
      "Now Iem when you see just we love when take you  \n",
      "You found the's unding's trised these dow to stranged ind.y, I've faded  \n",
      "Something againys  \n",
      "No the me, just behind  \n",
      "  \n",
      "I wanna not it int.  \n",
      "  \n",
      "Ip all rements 'roul and by \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 300000 iterations\n",
      "\n",
      "  \n",
      "They bold siys and by a plettire awarntoor  \n",
      "How I cantiled are braw will the smymeran, theal must always givering rame to this pun on the hah till elow  \n",
      "  \n",
      "Nawiono therongs  \n",
      "Oldber beh.  \n",
      "Ever they can't crave the biged smilis to do??, you ran  \n",
      "There aloness for the tritting the find phan't live there's no time againd schern  \n",
      "New would me what you're why be hered and therd thate all's and lose  \n",
      "As Yasibs your life the ersees and the sky you change as no all'ly life Kissed  \n",
      "Tell everythi \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 350000 iterations\n",
      "\n",
      " \n",
      "  \n",
      "Don't know.  \n",
      "May othund snow To falling and listand good, we more find save  \n",
      "Neel one cit  \n",
      "  \n",
      "But he bring  \n",
      "  \n",
      "My vilit lie to be at en by town fice  \n",
      "Ny\n",
      "Te're soving in a canned out tous  \n",
      "No\n",
      "\n",
      ", Donrany  \n",
      "He lord  \n",
      "That you make can marry listen liste  \n",
      "Iffool the night feel ghen pey fate how  \n",
      "We are for are of life for we and was foel  \n",
      "Now I'm risas misman  \n",
      "  \n",
      "My vell for swalp lick off away  \n",
      "Make my pirst mormoni listen - rounting though Mmobie  \n",
      "Nochide  \n",
      "I supside by like expepe \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 400000 iterations\n",
      "\n",
      " ok like a wornabll and you lies pra is just born whuld.  \n",
      "Just on my einghch with a brea-doding Marty band ait strild and honeating look dicking band  \n",
      "I wich a bleem of go ox  \n",
      "All sheres  \n",
      "No Tehiny, inside sobe alone  \n",
      "Some master above you take to ficy from the time and everyone's a better Is aing you'll let's lay,  \n",
      "Fa- gonna lilling dreambile alones alone of him is gone  \n",
      "All it up powuinding heal  \n",
      "All the mind a life all aroathot bream  \n",
      "Light.  \n",
      "And Agair  \n",
      "All like with like I have it  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 450000 iterations\n",
      "\n",
      "  \n",
      "A mounthing  \n",
      "And a deat and but now and got tricking  \n",
      "A good girke's nit moner  \n",
      "I gotuning  \n",
      "All no the ounatolbare  \n",
      "They shore up that nurss\"  \n",
      "It's thicking on up a true be comes wout beaterdour  \n",
      "When they piss this is it to gething look to the bank to the set canch on the couldon\" the band  \n",
      "White at the five\"\n",
      "\n",
      ", From John -onee \"Atso no coner Cages  \n",
      "In the wentifight  \n",
      "There's no mitile Bed wand  \n",
      "He seom to presertidis  \n",
      "Whene  \n",
      "Don't rust on my till and on sorth and home acout to D \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 500000 iterations\n",
      "\n",
      " me)  \n",
      "Pain home)  \n",
      "I can togethers, Gire, night nopis and he'll My friend teach on your blone (Oh, he's changed to do you ars, yes tommoreith to sayw... to me)  \n",
      "It's hersin' more only your love)  \n",
      "Ma every, tarry till  \n",
      "I wanl outhteciect to  \n",
      "(Chora you to believent you wi love to my hanon your home  \n",
      "They got the statesty, cares a-dil's can ba Chim  \n",
      "(Ye to the more and love can like you  \n",
      "It's popery........--her?)  \n",
      "I?me dones over,  \n",
      "He'cll Hisher, row when I frietors with me it of-yo here \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 550000 iterations\n",
      "\n",
      " s I'm feel, oh no morning?  \n",
      "About and all night froing like your lovation you far fear  \n",
      "  \n",
      "She's away  \n",
      "  \n",
      "Memn  \n",
      "Thruse  \n",
      "..  \n",
      "Lot alm my mirn  \n",
      "(could everybody love charside  \n",
      "  \n",
      "That's anybody  \n",
      "You s an forle you want.  \n",
      "Think I'm know  \n",
      "Alr oh, Been to Be love in the every done's no lot, for mina you  \n",
      "Mylding come to faring  \n",
      "Not!  \n",
      "  \n",
      "Chorus yeah yeah, you plaict to love to teada layings focker  \n",
      "You,  \n",
      "Can't be they know my rhis foelas  \n",
      "  \n",
      "We feel  \n",
      "It's lovin' do you life to helpant \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0  \n",
    "    \n",
    "    #select input sentence\n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "    \n",
    "    #select output sentence\n",
    "    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "    \n",
    "    #get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    #convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    \n",
    "    #train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n",
    "   \n",
    "       \n",
    "    #make predictions on every 500th iteration \n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        #length of characters we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        #randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        #sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = data[random_index:random_index + seq_length]\n",
    "    \n",
    "        #get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        #store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        #for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "        \n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            #convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            #compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n",
    "                                                      feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n",
    "\n",
    "            #we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            \n",
    "            #store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        #convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        #combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        #predict the predict text on every 50000th iteration\n",
    "        if iteration %50000 == 0:           \n",
    "            print ('\\n')\n",
    "            print (' After %d iterations' %(iteration))\n",
    "            print('\\n %s \\n' % (text,))   \n",
    "            print('-'*115)\n",
    "\n",
    "            \n",
    "    #increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow 1.13.1)",
   "language": "python",
   "name": "tf1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
